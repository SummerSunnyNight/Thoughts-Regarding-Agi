## 提出需求的能力：

人类有提出需求的能力，或者说，人类能提出符合人类需求的需求；机器也能模仿人提出一些符合人类需求的需求，机器还可以通过人的语言反馈/肢体动作猜测人的需求。但是这仍旧依赖于人去从源头获取需求，然后传达给机器。就像现在很多人的工作与其说是工作，到不如说更像是GPT调用机/prompt生成器。

从另一个角度来说，我们应该积极探索自己作为人类的需求，因为这是暂时人工智能无法取代的。

## RL与人的智能：

RL我认为是人的智能中的很重要的组成部分。我们可以把人的欲望作为奖励函数，人的思考/物理动作作为动作空间，人和世界的整体状态作为状态空间。

然而RL虽然能在围棋等少数任务中达到远超人类的水平，在实际工作中它却无法取代一个便宜的实习生。这是为什么呢？

## RL的局限性

### 奖励函数：

人的奖励函数是 好的感受（只是我个人的定义，不一定严谨）。然而好的感受的来源根本就是写在基因里的屎山，以现有技术很难提取成数学上的奖励函数。不过这件事在我看来属于难而正确的事。以2024年的人工智能研究方向来看，各大人工智能公司直接跳过了好的感受的来源的研究，而通过人工数据标注和rlhf来模拟这样的过程。但是我认为这样做短期虽然能取得成果，但是限制了人工智能的上限。理想的人工智能应该摆脱对标注数据的依赖，对ai tutor的依赖，直接从人类的原始欲望中进行奖励函数的计算。这样才能真正利用硅基比碳基计算速度快的优势。否则海量的算力投入下去，优化的结果也只能是人类社会中已经存在的一种艺术形式（甚至由于多模态数据的匮乏，尤其是人的感受的数据的匮乏，生成出来的结果甚至只能得其形而不能得其神）。

对于艺术（广义的）来说是这样，对于其它任务也会受到奖励函数的限制。这个过两天在写

奖励函数

### 动作空间：

To be continued

### 状态空间：

To be continued
